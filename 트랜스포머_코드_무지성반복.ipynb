{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXWm+in2DW84EWUiAsNdRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RealMyeong/Going_Deeper_NLP/blob/main/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8_%EC%BD%94%EB%93%9C_%EB%AC%B4%EC%A7%80%EC%84%B1%EB%B0%98%EB%B3%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 눈감고 코드 작성 가능할때까지 때려박자."
      ],
      "metadata": {
        "id": "N0HLS2pYWBD9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNPAcBoWVw6C"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(pos, d_model):\n",
        "  def cal_angle(position, i):\n",
        "    return position / np.power(10000, (2*int(i))/d_model)\n",
        "  \n",
        "  def get_posi_angle_vec(position):\n",
        "    return[cal_angle(position, i) for i in range(d_model)]\n",
        "  \n",
        "  sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
        "  sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
        "  sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
        "  return sinusoid_table\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    self.depth = d_model // self.num_heads\n",
        "    self.W_q = tf.keras.layers.Dense(d_model)\n",
        "    self.W_k = tf.keras.layers.Dense(d_model)\n",
        "    self.W_v = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.linear = tf.keras.layers.Dense(d_model)\n",
        "  \n",
        "  def scaled_dot_product_attention(self, Q, K, V, mask):\n",
        "    d_k = tf.cast(K.shape[-1], tf.float32)\n",
        "    QK = tf.matmul(Q, K, transpose_b=True)\n",
        "    sclaed_qk = QK / tf.math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      scaled_qk += (mask * 1e-9)\n",
        "    \n",
        "    attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
        "    out = tf.matmul(attentions, V)\n",
        "    return out, attentions\n",
        "  \n",
        "  def split_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    split_x = tf.transpose(split_x, perm=[0,2,1,3])\n",
        "    \n",
        "    return split_x\n",
        "  \n",
        "  def combine_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    combined_x = tf.transpose(x, perm=[0,2,1,3])\n",
        "    combined_x = tf.reshape(x, (batch_size, -1, self.d_model))\n",
        "\n",
        "    return combined_x\n",
        "  \n",
        "  def call(self, Q, K, V, mask):\n",
        "    WQ = self.W_q(Q)\n",
        "    WK = self.W_k(K)\n",
        "    WV = self.W_v(V)\n",
        "\n",
        "    WQ_split = self.split_heads(WQ)\n",
        "    WK_split = self.split_heads(WK)\n",
        "    WV_split = self.split_heads(WV)\n",
        "\n",
        "    out, attention_weights = self.scaled_dot_product_attention(WQ_split, WK_split, WV_split, mask)\n",
        "    out = self.combine_heads(out)\n",
        "    out = self.linear(out)\n",
        "\n",
        "    return out, attention_weights\n",
        "\n",
        "\n",
        "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    super(PoswiseFeedForwardNet, self).__init__()\n",
        "    self.W_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
        "    self.W_2 = tf.keras.layers.Dense(d_model)\n",
        "  \n",
        "  def call(self, x):\n",
        "    out = self.W_1(x)\n",
        "    out = self.W_2(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.enc_self_attention = MultiHeadAttention(d_model, n_heads)\n",
        "    self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "    self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "  \n",
        "  def call(self, x, mask):\n",
        "    residual = x\n",
        "    out = self.norm1(x)\n",
        "    out, enc_attn = self.enc_self_attention(out, out, out, mask)\n",
        "    out = self.dropout(out)\n",
        "    out += residual\n",
        "\n",
        "    residual = x\n",
        "    out = self.norm2(out)\n",
        "    out = self.ffn(out)\n",
        "    out = self.dropout(out)\n",
        "    out += residual\n",
        "\n",
        "    return out, enc_attn\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "    self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "    self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "  def call(self, x, enc_out, causality_mask, padding_mask):\n",
        "    residual = x\n",
        "    out = self.norm1(x)\n",
        "    out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
        "    out = self.dropout(out)\n",
        "    out += residual\n",
        "\n",
        "    residual = out\n",
        "    out = self.norm2(out))\n",
        "    out, enc_dec_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
        "    out = self.dropout(out)\n",
        "    out += residual\n",
        "\n",
        "    residual = out\n",
        "    out = self.norm3(out)\n",
        "    out = self.ffn(out)\n",
        "    out = self.dropout(out)\n",
        "    out += residaul\n",
        "\n",
        "    return out, dec_attn, enc_dec_attn\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               n_layers,\n",
        "               d_model,\n",
        "               n_heads,\n",
        "               d_ff,\n",
        "               dropout):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.n_layers = n_layers\n",
        "    self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
        "\n",
        "    def call(self, x, mask):\n",
        "      out = x\n",
        "      enc_attn = list()\n",
        "      for i in range(self.n_layers):\n",
        "        out, enc_attn = self.enc_layers[i](out, mask)\n",
        "        enc_attns.append(enc_attn)\n",
        "      \n",
        "      return out, enc_attn\n",
        "  \n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               n_layers,\n",
        "               d_model,\n",
        "               n_heads,\n",
        "               d_ff,\n",
        "               dropout):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.n_layers=n_layers\n",
        "    self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
        "\n",
        "  def call(self, x, enc_out, causality_mask, padding_mask):\n",
        "    out = x\n",
        "    dec_attns = list()\n",
        "    dec_enc_attns = list()\n",
        "    for i in range(self.n_layers):\n",
        "      out, dec_attns, enc_dec_attns = self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
        "\n",
        "      dec_attns.append(dec_attn)\n",
        "      dec_enc_attns(dec_enc_attn)\n",
        "    return out, dec_attns, dec_enc_attns\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               n_layers,\n",
        "               d_model,\n",
        "               n_heads,\n",
        "               d_ff,\n",
        "               src_vocab_size,\n",
        "               tgt_vocab_size,\n",
        "               pos_len,\n",
        "               dropout=0.2,\n",
        "               shared=True):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.d_model = tf.cast(d_model, tf.float32)\n",
        "    self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "    self.dec_emb = tf.keras.layers.Embedding(tat_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(pos_len, d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "    self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "    self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "    self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
        "    self.shared = shared\n",
        "    if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
        "  \n",
        "  def embedding(self, emb, x):\n",
        "    seq_len = x.shape[1]\n",
        "    out = emb(x)\n",
        "    if self.shared: out *= tf.math.sqrt(self.d_model)\n",
        "    out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
        "    out = self.dropout(out)\n",
        "\n",
        "    return out\n",
        "  \n",
        "  def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
        "    enc_in = self.embedding(self.enc_emb, enc_in)\n",
        "    dec_in = self.embedding(self.dec_emb, dec_in)\n",
        "    enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
        "    dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
        "    logits = self.fc(dec_out)\n",
        "    return logits, enc_attns, dec_attns, dec_enc_attns"
      ]
    }
  ]
}