{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EX_BS2_project6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ctxaoDY1cDbqeRLHyEcWCzJrTPnf2UlC",
      "authorship_tag": "ABX9TyNo/r77BmQuRy/M7t0RVl+D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RealMyeong/Aiffel_Exploration/blob/main/EX_BS2_project6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. 데이터 다운로드"
      ],
      "metadata": {
        "id": "PWIWJppmti6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 직접 다운로드 받아서 구글 드라이브에 저장함"
      ],
      "metadata": {
        "id": "008idEC5vxRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. 데이터 읽어오기"
      ],
      "metadata": {
        "id": "z4FTlvystn4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "txt_file_path = '/content/drive/MyDrive/AIFFEL/EX/data/lyrics/*'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "#여러개의 txt파일을 모두 읽어서 row_corpus에 저장\n",
        "for txt_file in txt_list:\n",
        "  with open(txt_file, 'r') as f:\n",
        "    raw = f.read().splitlines()\n",
        "    raw_corpus.extend(raw)\n",
        "\n",
        "print('데이터 크기: ',len(raw_corpus))\n",
        "print('Examples:\\n', raw_corpus[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXubHEa5vYJo",
        "outputId": "709be7df-b923-4b26-a486-fca15aae7f11"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기:  187088\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. 데이터 정제\n",
        "\n",
        "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n",
        "\n",
        "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n",
        "\n",
        "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 너무 긴 문장은 노래 가사 작사하기에 어울리지 않을 수도 있겠죠.\n",
        "그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기 를 권합니다."
      ],
      "metadata": {
        "id": "1DnpWWeFtq_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import re\n",
        "\n",
        "def preprocess_sentence(s):\n",
        "  s = s.lower().strip() #1 문장을 소문자로 바꾸고 양쪽 공백을 지움\n",
        "  s = re.sub(r\"([?.!,¿])\", r\" \\1\", s) #2 특수 문자 양쪽에 공백을 넣음\n",
        "  s = re.sub(r'[\" \"]+', \" \", s) #3 여러개의 공백을 하나의 공백으로 바꿈\n",
        "  s = re.sub(r'[^a-zA-Z?.!,¿]+', ' ', s) #4 a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿈\n",
        "  s = s.strip() #5 양쪽 공백을 지움\n",
        "  s = '<start> '+s+' <end>' #6 문장 시작에 <start> 끝에 <end> 넣어줌\n",
        "  return s"
      ],
      "metadata": {
        "id": "ibbXY01Lw_5I"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 여기에 정제된 문장을 모을겁니다\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    \n",
        "    # 정제를 하고 담아주세요\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "        \n",
        "# 정제된 결과를 10개만 확인해보죠\n",
        "corpus[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oKjZXQbFzmZP",
        "outputId": "c0c6706d-67fb-4c54-a644-39c9ff376341"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> made my way into the night <end>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=15000,\n",
        "                                                    filters=' ',\n",
        "                                                    oov_token='<unk>')\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  tensor = tokenizer.texts_to_sequences(corpus)\n",
        "  tensor_2 = []\n",
        "  print(tensor[0])\n",
        "  for i in range(len(tensor)): #토큰 개수가 15개 이하만 가져옴\n",
        "    if len(tensor[i]) <= 15:\n",
        "      tensor_2.append(tensor[i])\n",
        "\n",
        "  tensor_2 = tf.keras.preprocessing.sequence.pad_sequences(tensor_2, padding='post')\n",
        "  \n",
        "\n",
        "      \n",
        "  print(tensor_2, tokenizer)\n",
        "  return tensor_2, tokenizer\n",
        "\n",
        "tensor_2, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmI_wXds0-nn",
        "outputId": "20bbb678-21ce-45bf-8a63-87e66c392152"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 303, 28, 99, 4825, 3]\n",
            "[[  2 303  28 ...   0   0   0]\n",
            " [  2 221  13 ...   0   0   0]\n",
            " [  2  24  17 ...   0   0   0]\n",
            " ...\n",
            " [  2   3   0 ...   0   0   0]\n",
            " [  2   3   0 ...   0   0   0]\n",
            " [  2   3   0 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f5de394c0d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(tensor_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJn4YV_L2xxn",
        "outputId": "39b503f2-979d-492a-ef51-034ce417ea98"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6r69AxTUNLw",
        "outputId": "f28c59af-5faa-4ebf-e9c8-d8f9ab3ec22e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(156074, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUCb0_kO3z61",
        "outputId": "45eb33c5-d88d-423a-ca53-f6b483508410"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : ,\n",
            "5 : i\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : to\n",
            "10 : a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
        "src_input = tensor_2[:, :-1]  \n",
        "\n",
        "# tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
        "tgt_input = tensor_2[:, 1:]    \n",
        "\n",
        "print((src_input).shape)\n",
        "print((tgt_input).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtZmv1667TWL",
        "outputId": "469c0b7b-22cc-4ec6-9ea5-71dd67c14c3b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(156074, 14)\n",
            "(156074, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. 평가 데이터셋 분리\n",
        "\n",
        "훈련 데이터와 평가 데이터를 분리하세요!\n",
        "\n",
        "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상 으로 설정하세요! 총 데이터의 20% 를 평가 데이터셋으로 사용해 주세요!"
      ],
      "metadata": {
        "id": "EEKVmIENtuEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(src_input,\n",
        "                                                    tgt_input,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = 27)"
      ],
      "metadata": {
        "id": "nMJ2whjmXged"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5. 인공지능 만들기"
      ],
      "metadata": {
        "id": "jolOIPrbt4sK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        " # tokenizer가 구축한 단어사전 내 15000개와, 여기 포함되지 않은 0:<pad>를 포함하여 15001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만듦\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSD1iUZJYdu5",
        "outputId": "22c812eb-087c-44c4-8920-c6ec2f90a6df"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 14), dtype=tf.int32, name=None), TensorSpec(shape=(256, 14), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 256\n",
        "hidden_size = 1024\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "metadata": {
        "id": "2tDvBxnkYxIK"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
        "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "model(src_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHwH6U35Y2sn",
        "outputId": "69b90a3d-3b01-4fa7-d8b4-f20d73ae588d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 14, 15001), dtype=float32, numpy=\n",
              "array([[[-2.40244743e-04, -4.50904445e-05, -9.31438626e-06, ...,\n",
              "          6.98350923e-05,  1.08958469e-04, -1.39927170e-05],\n",
              "        [-5.58324449e-04,  1.80415464e-05,  8.26546056e-06, ...,\n",
              "         -1.35217051e-04,  1.24352646e-05,  1.76154335e-05],\n",
              "        [-6.98417658e-04,  9.52364135e-05,  3.38406069e-04, ...,\n",
              "         -4.96244640e-04, -2.88584288e-05,  1.13865999e-05],\n",
              "        ...,\n",
              "        [-1.33200188e-03,  4.90820501e-04, -1.33406941e-03, ...,\n",
              "         -1.57692842e-03, -2.13356502e-03, -1.09201006e-04],\n",
              "        [-1.39287277e-03,  7.23872101e-04, -1.89373200e-03, ...,\n",
              "         -1.72231521e-03, -2.46442342e-03,  3.56892451e-05],\n",
              "        [-1.44257385e-03,  9.40627884e-04, -2.39310367e-03, ...,\n",
              "         -1.84772233e-03, -2.76447763e-03,  1.67219056e-04]],\n",
              "\n",
              "       [[-2.40244743e-04, -4.50904445e-05, -9.31438626e-06, ...,\n",
              "          6.98350923e-05,  1.08958469e-04, -1.39927170e-05],\n",
              "        [-4.27659485e-04, -2.22595176e-04, -4.72166939e-05, ...,\n",
              "          7.92321862e-06,  2.47386546e-04,  4.62243861e-06],\n",
              "        [-5.31700847e-04, -2.58999877e-04, -2.04480282e-04, ...,\n",
              "          2.22338058e-05,  6.10947536e-05,  3.02170549e-04],\n",
              "        ...,\n",
              "        [-5.00205497e-04, -1.52562512e-03, -8.79268802e-04, ...,\n",
              "          6.43005653e-04, -1.85987781e-04,  2.53113359e-03],\n",
              "        [-6.60922495e-04, -1.36013411e-03, -8.72050412e-04, ...,\n",
              "          6.88425091e-04,  4.73501495e-05,  2.53754179e-03],\n",
              "        [-5.26840333e-04, -1.18127523e-03, -1.03136245e-03, ...,\n",
              "          4.41385026e-04,  2.85301794e-04,  2.59332755e-03]],\n",
              "\n",
              "       [[-2.40244743e-04, -4.50904445e-05, -9.31438626e-06, ...,\n",
              "          6.98350923e-05,  1.08958469e-04, -1.39927170e-05],\n",
              "        [-3.90150963e-04, -8.46835756e-05, -1.32644709e-04, ...,\n",
              "          3.87696258e-04,  2.10038008e-04,  1.97327841e-04],\n",
              "        [-5.76518883e-04, -6.96557163e-06, -2.54985353e-04, ...,\n",
              "          5.07127726e-04,  1.35341528e-04,  1.05030464e-04],\n",
              "        ...,\n",
              "        [-1.66328414e-03,  1.23508053e-03, -2.91612046e-03, ...,\n",
              "         -1.88221305e-03, -3.11405980e-03,  8.84329856e-05],\n",
              "        [-1.66588649e-03,  1.38194382e-03, -3.25531187e-03, ...,\n",
              "         -2.03522155e-03, -3.33516044e-03,  1.88196820e-04],\n",
              "        [-1.66326715e-03,  1.50736759e-03, -3.54753830e-03, ...,\n",
              "         -2.15662969e-03, -3.53059289e-03,  2.91009637e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-2.40244743e-04, -4.50904445e-05, -9.31438626e-06, ...,\n",
              "          6.98350923e-05,  1.08958469e-04, -1.39927170e-05],\n",
              "        [-3.62733670e-04, -1.72494721e-04, -1.84588935e-04, ...,\n",
              "          1.76108326e-04,  3.19734303e-04,  1.69562612e-04],\n",
              "        [-1.70852465e-04, -2.41563117e-04, -4.08358930e-04, ...,\n",
              "          6.77648059e-05,  6.64875668e-04,  3.56880046e-04],\n",
              "        ...,\n",
              "        [ 4.51067201e-04, -3.68137058e-04,  4.45469836e-04, ...,\n",
              "         -1.72077341e-03,  4.30300133e-04,  1.45232328e-03],\n",
              "        [ 3.37181293e-04, -4.22984129e-04,  7.48558436e-04, ...,\n",
              "         -1.93989731e-03,  9.33839328e-05,  1.23820978e-03],\n",
              "        [ 1.43427053e-04, -4.47605620e-04,  6.54836767e-04, ...,\n",
              "         -2.08070362e-03, -3.65235639e-04,  1.04568130e-03]],\n",
              "\n",
              "       [[-2.40244743e-04, -4.50904445e-05, -9.31438626e-06, ...,\n",
              "          6.98350923e-05,  1.08958469e-04, -1.39927170e-05],\n",
              "        [-2.57243402e-04, -2.02059717e-04,  1.55829082e-04, ...,\n",
              "         -1.55173839e-04,  1.27289619e-04, -3.58858058e-04],\n",
              "        [-4.63869492e-06, -1.27983643e-04,  1.59179850e-04, ...,\n",
              "         -4.60122596e-04,  5.11420658e-05, -5.48144162e-04],\n",
              "        ...,\n",
              "        [-9.87734180e-04,  1.09722884e-03, -2.91954144e-03, ...,\n",
              "         -2.22942769e-03, -3.20935552e-03,  1.85976998e-04],\n",
              "        [-1.10231957e-03,  1.26117514e-03, -3.24969622e-03, ...,\n",
              "         -2.31030886e-03, -3.41239199e-03,  2.86542199e-04],\n",
              "        [-1.19919097e-03,  1.40206120e-03, -3.53221921e-03, ...,\n",
              "         -2.37455964e-03, -3.59254330e-03,  3.84489802e-04]],\n",
              "\n",
              "       [[-2.40244743e-04, -4.50904445e-05, -9.31438626e-06, ...,\n",
              "          6.98350923e-05,  1.08958469e-04, -1.39927170e-05],\n",
              "        [-2.98097468e-04, -1.25273393e-04,  7.32498738e-05, ...,\n",
              "          1.12691057e-04, -3.03113939e-05, -5.29046374e-05],\n",
              "        [-2.73373269e-04, -1.79181108e-04,  9.64461142e-05, ...,\n",
              "          3.87778098e-04,  3.32591953e-05, -1.56419206e-04],\n",
              "        ...,\n",
              "        [-1.20305608e-03,  8.17412394e-04, -2.46739388e-03, ...,\n",
              "         -1.15064241e-03, -2.71601928e-03, -1.99317001e-04],\n",
              "        [-1.30247383e-03,  1.01359549e-03, -2.87976419e-03, ...,\n",
              "         -1.35156780e-03, -2.99148238e-03, -7.36558504e-05],\n",
              "        [-1.38427876e-03,  1.19269732e-03, -3.23806144e-03, ...,\n",
              "         -1.52688823e-03, -3.23093240e-03,  5.21260263e-05]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqib7CeyZA7p",
        "outputId": "da3edc8b-8c00-42c4-e892-70cd24da1131"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  3840256   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               multiple                  5246976   \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               multiple                  8392704   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  15376025  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 32,855,961\n",
            "Trainable params: 32,855,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "\n",
        "model.fit(dataset, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWMH1lKGZGhW",
        "outputId": "c5ffc5ca-9a21-41a8-8e08-f2c7c8c15556"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "609/609 [==============================] - 64s 99ms/step - loss: 3.4487\n",
            "Epoch 2/10\n",
            "609/609 [==============================] - 61s 99ms/step - loss: 2.9865\n",
            "Epoch 3/10\n",
            "609/609 [==============================] - 61s 99ms/step - loss: 2.8160\n",
            "Epoch 4/10\n",
            "609/609 [==============================] - 61s 99ms/step - loss: 2.6854\n",
            "Epoch 5/10\n",
            "609/609 [==============================] - 61s 99ms/step - loss: 2.5749\n",
            "Epoch 6/10\n",
            "609/609 [==============================] - 61s 99ms/step - loss: 2.4733\n",
            "Epoch 7/10\n",
            "609/609 [==============================] - 61s 99ms/step - loss: 2.3794\n",
            "Epoch 8/10\n",
            "609/609 [==============================] - 61s 99ms/step - loss: 2.2929\n",
            "Epoch 9/10\n",
            "609/609 [==============================] - 61s 99ms/step - loss: 2.2127\n",
            "Epoch 10/10\n",
            "609/609 [==============================] - 61s 99ms/step - loss: 2.1379\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5de608b950>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "id": "IfL2FAXhiVm6"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> money\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RA1xyX97ExeG",
        "outputId": "c1d1f86e-6c87-48f7-d6ff-f90d099d1652"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> money is the motivation that i ll be <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    }
  ]
}