{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EX_BS2_project6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ctxaoDY1cDbqeRLHyEcWCzJrTPnf2UlC",
      "authorship_tag": "ABX9TyNCRReCwoSorf2jqeW2jCUO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RealMyeong/Aiffel_Exploration/blob/main/EX_BS2_project6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1. 데이터 다운로드"
      ],
      "metadata": {
        "id": "PWIWJppmti6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 직접 다운로드 받아서 구글 드라이브에 저장함"
      ],
      "metadata": {
        "id": "008idEC5vxRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2. 데이터 읽어오기"
      ],
      "metadata": {
        "id": "z4FTlvystn4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "txt_file_path = '/content/drive/MyDrive/AIFFEL/EX/6. 작사가 만들기/data/lyrics/*'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "#여러개의 txt파일을 모두 읽어서 row_corpus에 저장\n",
        "for txt_file in txt_list:\n",
        "  with open(txt_file, 'r') as f:\n",
        "    raw = f.read().splitlines()\n",
        "    raw_corpus.extend(raw)\n",
        "\n",
        "print('데이터 크기: ',len(raw_corpus))\n",
        "print('Examples:\\n', raw_corpus[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXubHEa5vYJo",
        "outputId": "38c3a0f8-75be-4221-846c-9d200673d90c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 크기:  187088\n",
            "Examples:\n",
            " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. 데이터 정제\n",
        "\n",
        "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n",
        "\n",
        "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n",
        "\n",
        "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다. 너무 긴 문장은 노래 가사 작사하기에 어울리지 않을 수도 있겠죠.\n",
        "그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기 를 권합니다."
      ],
      "metadata": {
        "id": "1DnpWWeFtq_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import re\n",
        "\n",
        "def preprocess_sentence(s):\n",
        "  s = s.lower().strip() #1 문장을 소문자로 바꾸고 양쪽 공백을 지움\n",
        "  s = re.sub(r\"([?.!,¿])\", r\" \\1\", s) #2 특수 문자 양쪽에 공백을 넣음\n",
        "  s = re.sub(r'[\" \"]+', \" \", s) #3 여러개의 공백을 하나의 공백으로 바꿈\n",
        "  s = re.sub(r'[^a-zA-Z?.!,¿]+', ' ', s) #4 a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿈\n",
        "  s = s.strip() #5 양쪽 공백을 지움\n",
        "  s = '<start> '+s+' <end>' #6 문장 시작에 <start> 끝에 <end> 넣어줌\n",
        "  return s"
      ],
      "metadata": {
        "id": "ibbXY01Lw_5I"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 여기에 정제된 문장을 모을겁니다\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    \n",
        "    # 정제를 하고 담아주세요\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "        \n",
        "# 정제된 결과를 10개만 확인해보죠\n",
        "corpus[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oKjZXQbFzmZP",
        "outputId": "5afc90a3-2acc-466c-d958-4adbfb790f26"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> made my way into the night <end>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=15000,\n",
        "                                                    filters=' ',\n",
        "                                                    oov_token='<unk>')\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  tensor = tokenizer.texts_to_sequences(corpus)\n",
        "  tensor_2 = []\n",
        "  print(tensor[0])\n",
        "  for i in range(len(tensor)): #토큰 개수가 15개 이하만 가져옴\n",
        "    if len(tensor[i]) <= 15:\n",
        "      tensor_2.append(tensor[i])\n",
        "\n",
        "  tensor_2 = tf.keras.preprocessing.sequence.pad_sequences(tensor_2, padding='post')\n",
        "  \n",
        "\n",
        "      \n",
        "  print(tensor_2, tokenizer)\n",
        "  return tensor_2, tokenizer\n",
        "\n",
        "tensor_2, tokenizer = tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmI_wXds0-nn",
        "outputId": "5b97915d-4499-4c9c-fca3-92607e50a4d4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 303, 28, 99, 4825, 3]\n",
            "[[  2 303  28 ...   0   0   0]\n",
            " [  2 221  13 ...   0   0   0]\n",
            " [  2  24  17 ...   0   0   0]\n",
            " ...\n",
            " [  2   3   0 ...   0   0   0]\n",
            " [  2   3   0 ...   0   0   0]\n",
            " [  2   3   0 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7fe295748150>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(tensor_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJn4YV_L2xxn",
        "outputId": "042305d4-80b1-45c4-89d7-3a76b9988acc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6r69AxTUNLw",
        "outputId": "d8127490-921e-4b85-b385-919fc02717e6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(156074, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUCb0_kO3z61",
        "outputId": "4e3d2708-4c0a-41e8-c913-b1f7bf77a610"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : ,\n",
            "5 : i\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : to\n",
            "10 : a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
        "src_input = tensor_2[:, :-1]  \n",
        "\n",
        "# tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
        "tgt_input = tensor_2[:, 1:]    \n",
        "\n",
        "print((src_input).shape)\n",
        "print((tgt_input).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtZmv1667TWL",
        "outputId": "38d23d92-3ecb-4087-98dd-c595a5565151"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(156074, 14)\n",
            "(156074, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. 평가 데이터셋 분리\n",
        "\n",
        "훈련 데이터와 평가 데이터를 분리하세요!\n",
        "\n",
        "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상 으로 설정하세요! 총 데이터의 20% 를 평가 데이터셋으로 사용해 주세요!"
      ],
      "metadata": {
        "id": "EEKVmIENtuEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(src_input,\n",
        "                                                    tgt_input,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = 27)"
      ],
      "metadata": {
        "id": "nMJ2whjmXged"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5. 인공지능 만들기"
      ],
      "metadata": {
        "id": "jolOIPrbt4sK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        " # tokenizer가 구축한 단어사전 내 15000개와, 여기 포함되지 않은 0:<pad>를 포함하여 15001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만듦\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSD1iUZJYdu5",
        "outputId": "6281232d-8b54-4037-f6b1-a529dc8debd1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 14), dtype=tf.int32, name=None), TensorSpec(shape=(256, 14), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_3 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.rnn_3(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 512\n",
        "hidden_size = 1024\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "metadata": {
        "id": "2tDvBxnkYxIK"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
        "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "model(src_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHwH6U35Y2sn",
        "outputId": "785a5677-70cf-4950-d365-74e0053ca5bd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 14, 15001), dtype=float32, numpy=\n",
              "array([[[ 3.41715167e-06, -2.26322154e-05,  2.39533547e-05, ...,\n",
              "          8.53152233e-06, -6.53325478e-05,  6.31470584e-06],\n",
              "        [ 6.40008147e-06, -1.37129115e-04,  3.29638788e-05, ...,\n",
              "          9.07585636e-05, -1.46176346e-04,  2.59368062e-05],\n",
              "        [ 3.96689175e-05, -2.67870608e-04,  7.40196774e-05, ...,\n",
              "          1.87872851e-04, -2.63369991e-04,  1.01116326e-04],\n",
              "        ...,\n",
              "        [ 3.51427530e-04,  7.93124898e-04,  2.09736012e-04, ...,\n",
              "         -6.11118798e-04, -3.11780197e-04,  4.58414288e-04],\n",
              "        [ 2.97330698e-04,  8.53650097e-04,  1.72424363e-04, ...,\n",
              "         -7.53070053e-04, -2.84589245e-04,  3.90523433e-04],\n",
              "        [ 2.76017410e-04,  8.26149422e-04,  6.85479463e-05, ...,\n",
              "         -8.57305538e-04, -2.90537515e-04,  3.07581620e-04]],\n",
              "\n",
              "       [[ 3.41715167e-06, -2.26322154e-05,  2.39533547e-05, ...,\n",
              "          8.53152233e-06, -6.53325478e-05,  6.31470584e-06],\n",
              "        [-2.39332694e-05, -3.96018513e-05,  4.02370933e-05, ...,\n",
              "         -1.52552457e-05, -1.46793318e-04,  5.33516359e-05],\n",
              "        [-9.84110884e-05, -6.87018837e-06, -1.24506814e-05, ...,\n",
              "          1.48383715e-05, -2.12100480e-04,  1.31529552e-04],\n",
              "        ...,\n",
              "        [ 1.28627595e-04, -1.04129344e-04, -1.17592259e-04, ...,\n",
              "         -4.01830534e-04, -8.35088867e-05,  2.90813157e-04],\n",
              "        [ 1.73552064e-04, -3.36224010e-04, -1.81146068e-04, ...,\n",
              "         -4.13183880e-04, -1.51778833e-04,  2.31209619e-04],\n",
              "        [ 2.47594435e-04, -5.87613322e-04, -2.54517741e-04, ...,\n",
              "         -4.29843931e-04, -2.31411686e-04,  1.74177228e-04]],\n",
              "\n",
              "       [[ 3.41715167e-06, -2.26322154e-05,  2.39533547e-05, ...,\n",
              "          8.53152233e-06, -6.53325478e-05,  6.31470584e-06],\n",
              "        [-5.16205000e-05, -5.53445316e-05,  5.76887542e-05, ...,\n",
              "          1.76444792e-06, -1.41913697e-04, -1.78846904e-06],\n",
              "        [-1.08331318e-04, -1.10508183e-04,  9.50201720e-05, ...,\n",
              "         -7.27840816e-05, -1.88778882e-04,  5.88550029e-06],\n",
              "        ...,\n",
              "        [-7.36886635e-04,  6.03210588e-04,  1.57306404e-05, ...,\n",
              "         -1.04953593e-03, -6.51279755e-04,  2.76850362e-04],\n",
              "        [-6.52228598e-04,  6.12430566e-04, -8.65194088e-05, ...,\n",
              "         -1.08612014e-03, -7.12989538e-04,  2.45794014e-04],\n",
              "        [-4.92936873e-04,  5.49455348e-04, -2.14045343e-04, ...,\n",
              "         -1.09489786e-03, -7.64948782e-04,  1.96595167e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 3.41715167e-06, -2.26322154e-05,  2.39533547e-05, ...,\n",
              "          8.53152233e-06, -6.53325478e-05,  6.31470584e-06],\n",
              "        [ 1.59286992e-05, -2.72624802e-05,  3.94969902e-05, ...,\n",
              "         -4.86598874e-05, -1.29533946e-04,  9.28532900e-05],\n",
              "        [ 9.63965704e-06, -3.35283985e-05,  3.36686498e-05, ...,\n",
              "         -1.35476410e-04, -2.30720383e-04,  2.07623118e-04],\n",
              "        ...,\n",
              "        [-5.27826487e-04, -4.52526874e-04, -2.43815812e-04, ...,\n",
              "         -4.86940204e-04, -6.40310347e-04,  8.38722335e-04],\n",
              "        [-6.09888404e-04, -4.81161580e-04, -2.07398232e-04, ...,\n",
              "         -4.54079884e-04, -6.88469445e-04,  7.90855614e-04],\n",
              "        [-6.10619609e-04, -5.42939233e-04, -1.86750854e-04, ...,\n",
              "         -3.84114974e-04, -7.38581817e-04,  7.26364611e-04]],\n",
              "\n",
              "       [[ 3.41715167e-06, -2.26322154e-05,  2.39533547e-05, ...,\n",
              "          8.53152233e-06, -6.53325478e-05,  6.31470584e-06],\n",
              "        [-2.85972128e-05, -6.04340239e-05,  4.37762574e-05, ...,\n",
              "          1.49036341e-05, -1.49866522e-04,  7.31649270e-05],\n",
              "        [-6.13984084e-05, -1.21105135e-04,  1.70934600e-05, ...,\n",
              "          9.94768634e-05, -2.14011830e-04,  1.68957791e-04],\n",
              "        ...,\n",
              "        [-4.54846915e-04, -4.93978383e-04, -7.11420376e-04, ...,\n",
              "         -3.28383612e-04, -2.12467261e-04, -3.90137749e-04],\n",
              "        [-3.92266724e-04, -4.94547479e-04, -7.21765333e-04, ...,\n",
              "         -4.15667513e-04, -3.10919830e-04, -4.93186293e-04],\n",
              "        [-2.63289548e-04, -5.31323778e-04, -7.50861655e-04, ...,\n",
              "         -4.65084246e-04, -4.16590890e-04, -5.72215416e-04]],\n",
              "\n",
              "       [[ 3.41715167e-06, -2.26322154e-05,  2.39533547e-05, ...,\n",
              "          8.53152233e-06, -6.53325478e-05,  6.31470584e-06],\n",
              "        [ 2.68525582e-05, -2.34488525e-05,  4.34016656e-05, ...,\n",
              "          1.68771348e-05, -1.44321151e-04,  4.19074058e-05],\n",
              "        [ 6.03591579e-05, -1.98907710e-05,  3.25486944e-05, ...,\n",
              "          3.11057447e-05, -2.19879119e-04,  8.32186852e-05],\n",
              "        ...,\n",
              "        [-2.06907844e-05,  8.86045105e-04, -1.08116386e-04, ...,\n",
              "         -6.43471314e-04, -5.95386373e-04,  5.15058404e-04],\n",
              "        [ 5.83408564e-06,  1.05796056e-03,  6.04780416e-06, ...,\n",
              "         -7.39406736e-04, -5.88785217e-04,  4.01050143e-04],\n",
              "        [-1.30456274e-05,  1.20048807e-03,  1.22642377e-04, ...,\n",
              "         -8.83337052e-04, -6.06235524e-04,  2.93145393e-04]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqib7CeyZA7p",
        "outputId": "d407e800-55dc-49ae-93ca-e23da0fae01e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_generator_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  7680512   \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               multiple                  6295552   \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               multiple                  8392704   \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               multiple                  8392704   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  15376025  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 46,137,497\n",
            "Trainable params: 46,137,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=128, validation_data=(X_test, y_test), epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWMH1lKGZGhW",
        "outputId": "b59bdfdf-a20d-4d60-e955-e680098fedb0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "976/976 [==============================] - 91s 90ms/step - loss: 3.3589 - val_loss: 3.0980\n",
            "Epoch 2/10\n",
            "976/976 [==============================] - 83s 85ms/step - loss: 3.0016 - val_loss: 2.9831\n",
            "Epoch 3/10\n",
            "976/976 [==============================] - 83s 85ms/step - loss: 2.8691 - val_loss: 2.9020\n",
            "Epoch 4/10\n",
            "976/976 [==============================] - 86s 88ms/step - loss: 2.7713 - val_loss: 2.8489\n",
            "Epoch 5/10\n",
            "976/976 [==============================] - 86s 88ms/step - loss: 2.6763 - val_loss: 2.8031\n",
            "Epoch 6/10\n",
            "976/976 [==============================] - 86s 88ms/step - loss: 2.5938 - val_loss: 2.7657\n",
            "Epoch 7/10\n",
            "976/976 [==============================] - 86s 88ms/step - loss: 2.5159 - val_loss: 2.7346\n",
            "Epoch 8/10\n",
            "976/976 [==============================] - 86s 88ms/step - loss: 2.4425 - val_loss: 2.7144\n",
            "Epoch 9/10\n",
            "976/976 [==============================] - 86s 88ms/step - loss: 2.3731 - val_loss: 2.6971\n",
            "Epoch 10/10\n",
            "976/976 [==============================] - 83s 85ms/step - loss: 2.3060 - val_loss: 2.6872\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe2800dd0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ],
      "metadata": {
        "id": "IfL2FAXhiVm6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i hate\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RA1xyX97ExeG",
        "outputId": "e459003f-30ed-4559-f638-5275aeb16532"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i hate to be startin to <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}